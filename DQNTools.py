import numpy as npimport torchfrom torch import nn as nnimport gymimport osfrom network_utils import np2torchfrom base_gaussian_policy import GaussianToolPolicyfrom environment.simulator import ToolEnvimport matplotlib.pyplot as pltimport argparseimport imageioimport tqdmfrom collections import dequefrom PIL import ImageDEVICE = "cuda" if torch.cuda.is_available() else "cpu"END_TOOLS = 3END_ROWS = 25END_COLS = 25INPUT_STATE_SHAPE = (7,600,600)NITERS = 800def compute_linear_input(height, width, conv_layers, model_version):    mod_channels = 32/(2 ** (conv_layers-1))    if model_version == 'base':        mod_height = height - (2 * conv_layers)        mod_width = width - (2 * conv_layers)    else:        mod_height = height/(2 ** conv_layers)        mod_width = width/(2 ** conv_layers)    return int(round(mod_channels * mod_height * mod_width))def add_coord_conv_channels_np(img_arr):    _, rows, cols = np.shape(img_arr)    base_row = np.reshape(np.arange(cols) + 1, (1,-1))    ones_arr_rows = np.zeros((rows,1)) + 1    x_coord_channel = np.expand_dims(np.matmul(ones_arr_rows, base_row), axis=0)    y_coord_channel = np.expand_dims(np.matmul(base_row.T, ones_arr_rows.T), axis=0)    #print(np.shape(x_coord_channel))    #print(np.shape(y_coord_channel))    return np.concatenate((x_coord_channel[:,::-1], y_coord_channel[::-1],x_coord_channel, y_coord_channel, img_arr), axis = 0)def to_image_np(state):    return np.swapaxes(np.swapaxes(state, 0, 1), 1, 2)def to_state_np(image_arr):    return np.swapaxes(np.swapaxes(image_arr, 1, 2), 0, 1)def to_image_torch(state):    return torch.swapaxes(torch.swapaxes(state, 0, 1), 1, 2)def to_state_torch(image_arr):    return torch.swapaxes(torch.swapaxes(image_arr, 1, 2), 0, 1)def save_arr_to_file(image_arr, filepath):    into_file_format = image_arr.astype(np.uint8)    im = Image.fromarray(into_file_format)    im.save(filepath)    def row_col_to_x_y(row, col):    return float(col - END_ROWS//2)/(END_COLS/2), -float(row - END_COLS//2)/(END_ROWS/2)def row_col_to_index(row, col):    return row * END_COLS + coldef index_to_row_col(index):    row = int(round(index))//END_COLS    col = int(round(index))%END_COLS    return int(row), int(col)def index_to_tool_row_col(index):    tool = index//(END_ROWS * END_COLS)    mod_index = index%(END_ROWS * END_COLS)    row, col = index_to_row_col(int(mod_index))    return int(tool), int(row), int(col)def plot_heatmap(arr, filepath):    plt.clf()    plt.imshow(arr, cmap='hot', interpolation='nearest')    plt.savefig(filepath)    plt.clf()    def tool_level_rewards_filepath(tool_num: int, level: int):    return "data/level_tool_rewards/" + str(int(level)) + "_" + str(int(tool_num)) + "_" + str(END_ROWS) + "_" + str(END_COLS) + ".txt"class Q_Network(nn.Module):    def __init__(self, state_input_size, num_actions):        super().__init__()        (channels, height, width) = state_input_size        linear_medium_size = 200 * 200                print("in to linear layer size: " + str(compute_linear_input(height, width, 3, 'conv')))                layers_small = [nn.Conv2d(channels, 12, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.MaxPool2d(kernel_size = 2, stride = 2),                         nn.Conv2d(12, 6, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.MaxPool2d(kernel_size = 2, stride = 2),                         nn.Conv2d(6, 3, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.Flatten(start_dim = 0),                         nn.Linear(3*150*150,3*50*50),                         nn.ReLU(),                         nn.Linear(3*50*50,3*25*25)]                """        layers_shrink = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(16, 8, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(8, 4, kernel_size = 3, stride = 1, padding = 'same'),                   nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 3, 'conv'), linear_medium_size),                   nn.ReLU(),                   nn.Linear(linear_medium_size, num_actions)                  ]                        layers_3conv = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(16, 8, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(8, 4, kernel_size = 3, stride = 1, padding = 'same'),                   nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 3, 'conv'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]                layers_2conv = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(16, 8, kernel_size = 3, stride = 1, padding = 'same'),                   nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 2, 'conv'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]                layers_1conv = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                    nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 1, 'conv'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]                layers_base = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 0),                    nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 1, 'base'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]        """        self.learning_rate = 0.1        self.model = nn.Sequential(*layers_small)        self.model = self.model.to(DEVICE)        self.optimizer = torch.optim.Adam(params = self.model.parameters(), lr = self.learning_rate)        self.num_actions = num_actions        def forward(self, input_state):        input_state = input_state.to(DEVICE)        return self.model(input_state)        def backprop_single(self, state, action, true_value):        self.optimizer.zero_grad()        computed_value = torch.sum(self.model(state) * torch.nn.functional.one_hot(torch.tensor(action).to(torch.int64), self.num_actions))        mse_loss = (computed_value - true_value) ** 2        mse_loss.backward()        self.optimizer.step()        return mse_loss.item()        def backprop_batch_full(self, state, all_rewards, tool_num: int):        self.optimizer.zero_grad()        action_values = self.model(state)        pixels = int(END_ROWS * END_COLS)        action_values = action_values[tool_num*pixels:(tool_num+1)*pixels]        loss = 0        for index in range(len(action_values)):            row, col = index_to_row_col(index)            true_reward = all_rewards[row][col]            comp_reward = action_values[index]            loss += ((true_reward - comp_reward) ** 2)        loss.backward()        self.optimizer.step()        return loss.item()class DQN(object):    """    Class for implementing a DQN    """    def __init__(self, env, seed, exp_dir, name, curr_level):        self.env = env        #self.env.seed(self.seed)        # self.lr = 3e-2        self.lr = 0.1 #0.1 #1.5 #0.5        self.exp_dir = exp_dir        self.name = name                self.state_shape = INPUT_STATE_SHAPE        self.num_actions = END_ROWS * END_COLS        self.level = curr_level    def copy_to_target(self):        self.target_network.model.load_state_dict(self.Q_network.model.state_dict())            def get_action_values(self, input_state, model_type = 'target'):        if model_type == 'target' or model_type == 'target_network':            return self.target_network(input_state)        elif model_type == 'q_network':            return self.Q_network(input_state)        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")                def backprop_single(self, state, action, target_value, model_type = 'q_network'):        if model_type == 'q_network':            use_model = self.Q_network        elif model_type == 'target' or model_type == 'target_network':            use_model = self.target_network        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")            print("FOR BACKPROP THERE ISN'T ANY REASON TO PASS THIS PARAMETER IN")                    loss = use_model.backprop_single(state, action, target_value)        return loss        def backprop_batch_full(self, state, all_rewards, tool_num: int, model_type = 'q_network'):        if model_type == 'q_network':            use_model = self.Q_network        elif model_type == 'target' or model_type == 'target_network':            use_model = self.target_network        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")            print("FOR BACKPROP THERE ISN'T ANY REASON TO PASS THIS PARAMETER IN")                    loss = use_model.backprop_batch_full(state, all_rewards, int(tool_num))        return loss        def init_model(self):        self.Q_network = Q_Network(self.state_shape, self.num_actions)        #self.target_network = Q_Network(self.state_shape, self.num_actions)        def init_averages(self):        """        You don't have to change or use anything here.        """        self.avg_reward = 0.0        self.max_reward = 0.0        self.std_reward = 0.0        self.eval_reward = 0.0            def single_run_full(self, tool_num, pos_x, pos_y):        env.reset()        img_arr = env.img.copy()                save_arr_to_file(img_arr, "pre_tool_env.png")                action = np.array([tool_num,pos_x,pos_y])        print("Action: ", action)                reward = env.step(action)        print("Reward" + str(reward))                post_tool_imgs = env.render()                for index in range(len(post_tool_imgs)):            filepath = "temp_imgs/img-" + str(index) + ".png"            curr_img = post_tool_imgs[index]            save_arr_to_file(curr_img, filepath)                    post_tool_img_arr = post_tool_imgs[-1]        #print(post_tool_img_arr)        #print(np.shape(post_tool_img_arr))        save_arr_to_file(post_tool_img_arr, "post_tool_env.png")                env.reset()            def sample_and_train(self, tool_num, row, col):        env.reset()        img_arr = env.img.copy()                        np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()                x, y = row_col_to_x_y(row, col)        action = np.array([int(round(tool_num)), x, y])                reward = env.step(action)        if reward is None:            reward = -1                    reward *= 1000        if reward > 0:            reward *= 10                index = row_col_to_index(row, col)        loss = self.backprop_single(torch_state, index, reward)                return loss        def sample_only(self, tool_num, row, col):        env.reset()                x, y = row_col_to_x_y(row, col)        #print(x,y)        action = np.array([int(round(tool_num)), x, y])                reward = env.step(action)        if reward is None:            reward = -1                    reward *= 1000        if reward > 0:            reward *= 10                    return reward        def sample_all_rewards(self, tool_num: int):        results = np.zeros((END_ROWS, END_COLS))        print("SAMPLING ALL REWARDS")        for row in range(END_ROWS):            print()            print("Row: " + str(row))            print("Cols: ")            for col in range(END_COLS):                if col%15 == 0:                    print(col)                reward = self.sample_only(tool_num, row, col)                results[row][col] = reward        return results        def sample_all_n_times(self, niters: int, tool_num: int):        for iteration in range(1,niters+1):            print("Iteration: ", iteration)            for row in range(80):                print(row)                for col in range(80):                    self.sample_and_train(tool_num, row, col)            for row in range(80,150,5):                print(row)                for col in range(80,150,5):                    self.sample_and_train(tool_num, row, col)                      def modify_rewards(self, np_arr):        #np_arr[np_arr == 0] = -5        #np_arr[np_arr == -1000] = -10        #np_arr[np_arr == 10000] = 25        return np_arr                        def train_n_iters(self, niters: int, tool_num: int):        level = int(self.level)        filepath = tool_level_rewards_filepath(tool_num, level)        if not(os.path.exists(filepath)):            print(filepath, " NOT FOUND")            rewards = self.sample_all_rewards(tool_num)            np.savetxt(filepath, rewards)                use_rewards = np.loadtxt(filepath)        mod_rewards = self.modify_rewards(use_rewards)        all_rewards = np2torch(mod_rewards)                env.reset()        img_arr = env.img.copy()                        np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()                for iteration in range(1, niters+1):            print("Backproping iteration: " + str(iteration))            loss = self.backprop_batch_full(torch_state, all_rewards, tool_num)            print("Loss: " + str(loss))                                        def collect_heat_map(self, tool_num: int):        env.reset()        img_arr = env.img.copy()                        np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()                all_action_values = self.get_action_values(torch_state, model_type = "q_network")        #print(all_action_values.size())        pixels = END_ROWS * END_COLS        all_action_values = all_action_values[tool_num * pixels:(tool_num+1)*pixels]        expanded_arr = np.zeros((600,600))                rows = END_ROWS        cols = END_COLS        ratio_r = int(600//rows)        ratio_c = int(600//cols)                for index, value_item in enumerate(all_action_values):            row, col = index_to_row_col(index)            srow = row*ratio_r            scol = col*ratio_c            for ar in range(ratio_r):                for ac in range(ratio_c):                    expanded_arr[srow+ar][scol+ac] = value_item.item()                            env.reset()        return expanded_arr        def sample_best(self, tool_num: int):        env.reset()        img_arr = env.img.copy()                save_arr_to_file(img_arr, "pre_tool_env.png")                np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()        action_values = self.Q_network(torch_state)        pixels = END_ROWS * END_COLS        action_values = action_values[tool_num * pixels : (tool_num + 1) * pixels]        action_values_np = action_values.detach().numpy()        np.savetxt("actvals.txt", action_values_np)        best_index = torch.argmax(action_values).item()        best_row, best_col = index_to_row_col(best_index)        best_x, best_y = row_col_to_x_y(best_row, best_col)                print("BEST POSITION RUN INFORMATION")        self.single_run_full(tool_num, best_x, best_y)            def save_all(self, tool_num: int, NITER_VAL, level_val):        heatmap = self.collect_heat_map(tool_num)        temp_filepath_heatmap_vals = "data/heatmapDataTool/heatmapvals_tool_" + str(int(tool_num)) + "_" + str(int(level_val)) + "_" + str(int(NITER_VAL)) + ".txt"        temp_filepath_params = "data/modelparamsTool/model_params_tool_" + str(int(tool_num)) + "_" + str(int(level_val)) + "_" + str(int(NITER_VAL)) + ".txt"        temp_filepath_heatmap_png = "data/heatmapDataTool/heatmap_tool_" + str(int(tool_num)) + "_"  + str(int(level_val)) + "_" + str(int(NITER_VAL)) + ".png"                np.savetxt(temp_filepath_heatmap_vals, heatmap)        torch.save(self.Q_network.state_dict(), temp_filepath_params)        plot_heatmap(heatmap, temp_filepath_heatmap_png)            def run(self):        """        Apply procedures of training for a PG.        """                self.init_model()        self.init_averages()        self.single_run_full(0,-0.8,0.8)        self.Q_network.load_state_dict(torch.load("data/modelparamsTool/model_params_tool_0_-2_50.txt"))                """        self.save_all(0, 0, self.level)        self.save_all(1, 0, self.level)        self.save_all(2, 0, self.level)                for i in range(1,6):            TEMP_NITERS = 10            TOTAL_ITERS = i * 10                        if i == 1:                for j in range(10):                    self.train_n_iters(niters = 1, tool_num = 0)                    self.save_all(tool_num = 0, NITER_VAL = j+1, level_val = self.level)                    self.train_n_iters(niters = 1, tool_num = 1)                    self.save_all(tool_num = 1, NITER_VAL = j+1, level_val = self.level)                    self.train_n_iters(niters = 1, tool_num = 2)                    self.save_all(tool_num = 2, NITER_VAL = j+1, level_val = self.level)                continue                        for tn in range(3):                self.train_n_iters(niters = TEMP_NITERS, tool_num = tn)                self.save_all(tool_num = tn, NITER_VAL = TOTAL_ITERS, level_val = self.level)        """        self.sample_best(0)        self.sample_best(1)        self.sample_best(2)        #self.single_run()                #temp_filepath_heatmap_vals = "data/heatmapData/heatmapvals_tool0_" + str(int(self.level)) + "_" + str(NITERS) + ".txt"        #temp_filepath_params = "data/modelparams/model_params_tool0_" + str(int(self.level)) + "_" + str(NITERS) + ".txt"        #temp_filepath_heatmap_png = "data/heatmapData/heatmap_tool0_pic_"  + str(int(self.level)) + "_" + str(NITERS) + ".png"                #np.savetxt(temp_filepath_heatmap_vals, heatmap)        #torch.save(self.Q_network.state_dict(), temp_filepath_params)        #plot_heatmap(heatmap, temp_filepath_heatmap_png)                # record one game at the beginning        # model        #self.init_policy()        #self.init_averages()        #self.train()        # record one game at the endif __name__ == "__main__":    parser = argparse.ArgumentParser()    parser.add_argument(        "--seed",        type=int,        required = False,        default=1,        help="seed",    )    parser.add_argument(        "--counterfactual",        action='store_true',        help="counterfactual",    )    parser.add_argument(        "--shaped_reward",        action='store_true',        help="shaped reward",    )    parser.add_argument(        "--object_prior",        action='store_true',        help="object prior",    )    parser.add_argument(        "--baseline",        action='store_true',        help="subtract average reward",    )    parser.add_argument(        "--epochs",        type=int,        required=False,        default=100,        help="training_length",    )    parser.add_argument(        "--batch_size",        type=int,        required=False,        default=10,        help="batch size ",    )    parser.add_argument(        "--eval_frq",        type=int,        required=False,        default=5,        help="batch size",    )    parser.add_argument(        "--eval_trials",        type=int,        required=False,        default=20,        help="number of times to run during eval",    )    parser.add_argument(        "--level",        type=int,        required=False,        default=0,        help="level of the game you play",    )    parser.add_argument(        "--name",        type=str,        required=False,        default="full_algorithm",        help="name of algorithm run",    )    parser.add_argument(        "--exp_dir",        type=str,        required=False,        default="experiments/initial_tests",        help="name of algorithm run",    )    args = parser.parse_args()    env = ToolEnv(environment = args.level, shaped = args.shaped_reward)    dqn = DQN(env, seed = args.seed, exp_dir = args.exp_dir,                        name = args.name, curr_level = args.level)    dqn.run()