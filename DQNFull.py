import numpy as npimport torchfrom torch import nn as nnimport gymimport osfrom network_utils import np2torchfrom base_gaussian_policy import GaussianToolPolicyfrom environment.simulator import ToolEnvimport matplotlib.pyplot as pltimport argparseimport imageioimport tqdmfrom collections import dequefrom PIL import ImageDEVICE = "cuda" if torch.cuda.is_available() else "cpu"END_ROWS = 25END_COLS = 25INPUT_STATE_SHAPE = (7,600,600)NITERS = 10def compute_linear_input(height, width, conv_layers, model_version):    mod_channels = 32/(2 ** (conv_layers-1))    if model_version == 'base':        mod_height = height - (2 * conv_layers)        mod_width = width - (2 * conv_layers)    else:        mod_height = height/(2 ** conv_layers)        mod_width = width/(2 ** conv_layers)    return int(round(mod_channels * mod_height * mod_width))def add_coord_conv_channels_np(img_arr):    _, rows, cols = np.shape(img_arr)    base_row = np.reshape(np.arange(cols) + 1, (1,-1))    ones_arr_rows = np.zeros((rows,1)) + 1    x_coord_channel = np.expand_dims(np.matmul(ones_arr_rows, base_row), axis=0)    y_coord_channel = np.expand_dims(np.matmul(base_row.T, ones_arr_rows.T), axis=0)    #print(np.shape(x_coord_channel))    #print(np.shape(y_coord_channel))    return np.concatenate((x_coord_channel[:,::-1], y_coord_channel[::-1],x_coord_channel, y_coord_channel, img_arr), axis = 0)def to_image_np(state):    return np.swapaxes(np.swapaxes(state, 0, 1), 1, 2)def to_state_np(image_arr):    return np.swapaxes(np.swapaxes(image_arr, 1, 2), 0, 1)def to_image_torch(state):    return torch.swapaxes(torch.swapaxes(state, 0, 1), 1, 2)def to_state_torch(image_arr):    return torch.swapaxes(torch.swapaxes(image_arr, 1, 2), 0, 1)def save_arr_to_file(image_arr, filepath):    into_file_format = image_arr.astype(np.uint8)    im = Image.fromarray(into_file_format)    im.save(filepath)    def row_col_to_x_y(row, col):    return float(col - END_ROWS//2)/(END_COLS/2), -float(row - END_COLS//2)/(END_ROWS/2)def row_col_to_index(row, col):    return row * END_COLS + coldef index_to_row_col(index):    row = int(round(index))//END_COLS    col = int(round(index))%END_COLS    return int(row), int(col)def plot_heatmap(arr, filepath):    plt.clf()    plt.imshow(arr, cmap='hot', interpolation='nearest')    plt.savefig(filepath)    plt.clf()    def tool_level_batch_rewards_filepath(tool_num: int, level: int):    return "data/level_tool_batch/rewards_level_" + str(int(level)) + "_tool_" + str(int(tool_num)) + ".txt"def tool_level_batch_states_filepath(tool_num: int, level: int):    return "data/level_tool_batch/states_level_" + str(int(level)) + "_tool_" + str(int(tool_num)) + ".txt"    def tool_level_rewards_filepath(tool_num: int, level: int):    return "data/level_tool_rewards/" + str(int(level)) + "_" + str(int(tool_num)) + "_" + str(END_ROWS) + "_" + str(END_COLS) + ".txt"def tool_level_row_col_filepath(tool_num: int, level: int, row: int, col: int):    return "data/level_tool_states/" + str(int(level)) + "/" + str(int(tool_num)) + "_" + str(row) + "_" + str(col) + "_" + str(END_ROWS) + "_" + str(END_COLS) + ".txt"class Q_Network(nn.Module):    def __init__(self, state_input_size, num_actions):        super().__init__()        (channels, height, width) = state_input_size                print("in to linear layer size: " + str(compute_linear_input(height, width, 3, 'conv')))                layers_small = [nn.Conv2d(channels, 4, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.MaxPool2d(kernel_size = 2, stride = 2),                         nn.Conv2d(4, 2, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.MaxPool2d(kernel_size = 2, stride = 2),                         nn.Conv2d(2, 1, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.Flatten(),                         nn.Linear(150*150,100*100),                         nn.ReLU(),                         nn.Linear(100*100,50*50),                        nn.ReLU(),                        nn.Linear(50*50,1)]                self.learning_rate = 0.1        self.model = nn.Sequential(*layers_small)        self.model = self.model.to(DEVICE)        self.optimizer = torch.optim.Adam(params = self.model.parameters(), lr = self.learning_rate)        self.num_actions = num_actions        def forward(self, input_state):        input_state = input_state.to(DEVICE)        return self.model(input_state)        def backprop_single(self, state, true_value):        self.optimizer.zero_grad()        computed_value = self.model(state)        mse_loss = (computed_value - true_value) ** 2        mse_loss.backward()        self.optimizer.step()        return mse_loss.item()        def backprop_batch_full(self, batch_states, batch_rewards):        self.optimizer.zero_grad()                pred_rewards = self.model(batch_states)        batch_rewards = batch_rewards.unsqueeze(dim = 1)                assert pred_rewards.size(dim = 0) == batch_states.size(dim = 0)        assert pred_rewards.size() == batch_rewards.size()                loss = torch.sum(torch.square(pred_rewards - batch_rewards))                loss.backward()        self.optimizer.step()        return loss.item()class DQN(object):    """    Class for implementing a DQN    """    def __init__(self, env_list, exp_dir, name, levels_list, test_env_list, test_levels_list):        self.env_list = env_list        #self.env.seed(self.seed)        # self.lr = 3e-2        self.lr = 0.1 #0.1 #1.5 #0.5        self.exp_dir = exp_dir        self.name = name                self.state_shape = INPUT_STATE_SHAPE        self.num_actions = END_ROWS * END_COLS        self.levels_list = levels_list                self.test_env_list = test_env_list        self.test_levels_list = test_levels_list                self.level_tool_states  = [[None for _ in range(3)] for k in self.levels_list]        self.level_tool_rewards = [[None for _ in range(3)] for k in self.levels_list]    def copy_to_target(self):        self.target_network.model.load_state_dict(self.Q_network.model.state_dict())            def get_action_values(self, input_state, model_type = 'target'):        if model_type == 'target' or model_type == 'target_network':            return self.target_network(input_state)        elif model_type == 'q_network':            return self.Q_network(input_state)        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")                def backprop_single(self, state, target_value, model_type = 'q_network'):        if model_type == 'q_network':            use_model = self.Q_network        elif model_type == 'target' or model_type == 'target_network':            use_model = self.target_network        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")            print("FOR BACKPROP THERE ISN'T ANY REASON TO PASS THIS PARAMETER IN")                    loss = use_model.backprop_single(state, target_value)        return loss        def backprop_batch_full(self, batch_states, batch_rewards, model_type = 'q_network'):        if model_type == 'q_network':            use_model = self.Q_network        elif model_type == 'target' or model_type == 'target_network':            use_model = self.target_network        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")            print("FOR BACKPROP THERE ISN'T ANY REASON TO PASS THIS PARAMETER IN")                    loss = use_model.backprop_batch_full(batch_states, batch_rewards)        return loss        def init_model(self):        self.Q_network = Q_Network(self.state_shape, self.num_actions)        #self.target_network = Q_Network(self.state_shape, self.num_actions)        def single_run_full(self, level_index, tool_num, pos_x, pos_y):        env = self.env_list[level_index]        env.reset()        img_arr = env.img.copy()                save_arr_to_file(img_arr, "pre_tool_env.png")                action = np.array([tool_num,pos_x,pos_y])        print("Action: ", action)                reward = env.step(action)        print("Reward" + str(reward))                post_tool_imgs = env.render()                for index in range(len(post_tool_imgs)):            filepath = "temp_imgs/img-" + str(index) + ".png"            curr_img = post_tool_imgs[index]            save_arr_to_file(curr_img, filepath)                    post_tool_img_arr = post_tool_imgs[-1]        save_arr_to_file(post_tool_img_arr, "post_tool_env.png")                env.reset()    def modify_rewards(self, np_arr):        return np_arr        def sample_only(self, level_index, tool_num, row, col):        env = self.env_list[level_index]        env.reset()                x, y = row_col_to_x_y(row, col)        action = np.array([int(round(tool_num)), x, y])                reward = env.step(action)        if reward is None:            reward = -1                    reward *= 1000        if reward > 0:            reward *= 10                    return reward        def sample_all_rewards(self, level_index: int, tool_num: int):        results = np.zeros((END_ROWS, END_COLS))        print("SAMPLING ALL REWARDS")        for row in range(END_ROWS):            print()            print("Row: " + str(row))            print("Cols: ")            for col in range(END_COLS):                if col%15 == 0:                    print(col)                reward = self.sample_only(level_index, tool_num, row, col)                results[row][col] = reward        return results        def create_tool_level_state(self, tool_num: int, level_index: int, row: int, col: int):                self.sample_only(level_index, tool_num, row, col)                img_arr = self.env_list[level_index].render()[0]        state_arr = to_state_np(img_arr)                return add_coord_conv_channels_np(state_arr)                                def train_n_iters(self, niters: int, level_index: int, tool_num: int):        level_val = self.levels_list[level_index]                filepath = tool_level_rewards_filepath(tool_num, level_val)        if not(os.path.exists(filepath)):            rewards = self.sample_all_rewards(level_index, tool_num)            np.savetxt(filepath, rewards)                use_rewards = np.loadtxt(filepath)        mod_rewards = self.modify_rewards(use_rewards)        all_rewards = np2torch(mod_rewards)                all_states = []        corres_rewards = []                if self.level_tool_states[level_index][tool_num] is None:            for row in range(END_ROWS):                for col in range(END_COLS):                    if all_rewards[row][col] < 0:                        continue                                        curr_reward = all_rewards[row][col]                                        temp_filepath = tool_level_row_col_filepath(tool_num, level_val, row, col)                    if not(os.path.exists(temp_filepath)):                        state_arr = self.create_tool_level_state(tool_num, level_index, row, col)                        np.savetxt(temp_filepath, np.reshape(state_arr, (INPUT_STATE_SHAPE[0], INPUT_STATE_SHAPE[1] * INPUT_STATE_SHAPE[2])))                                            curr_state_arr = self.load_state_np(temp_filepath)                    state_arr_torch = np2torch(curr_state_arr)                                        all_states.append(state_arr_torch)                    corres_rewards.append(curr_reward)                                self.level_tool_states[level_index][tool_num] = torch.stack(all_states, dim = 0)            self.level_tool_rewards[level_index][tool_num] = torch.tensor(corres_rewards)                                        batch_states = self.level_tool_states[level_index][tool_num]        batch_rewards =  self.level_tool_rewards[level_index][tool_num]                for iteration in range(1, niters+1):            print("Backproping iteration: " + str(iteration))            loss = self.backprop_batch_full(batch_states, batch_rewards)            print("Loss: " + str(loss))                def collect_level_tool_states_rewards(self, level_index: int, tool_num: int):        level_val = self.levels_list[level_index]                filepath = tool_level_rewards_filepath(tool_num, level_val)        if not(os.path.exists(filepath)):            rewards = self.sample_all_rewards(level_index, tool_num)            np.savetxt(filepath, rewards)                use_rewards = np.loadtxt(filepath)        mod_rewards = self.modify_rewards(use_rewards)        all_rewards = np2torch(mod_rewards)                all_states = []        corres_rewards = []                if self.level_tool_states[level_index][tool_num] is None:            for row in range(END_ROWS):                for col in range(END_COLS):                    print(row,col)                    if all_rewards[row][col] < 0:                        continue                                        curr_reward = all_rewards[row][col]                                        temp_filepath = tool_level_row_col_filepath(tool_num, level_val, row, col)                    if not(os.path.exists(temp_filepath)):                        state_arr = self.create_tool_level_state(tool_num, level_index, row, col)                        np.savetxt(temp_filepath, np.reshape(state_arr, (INPUT_STATE_SHAPE[0], INPUT_STATE_SHAPE[1] * INPUT_STATE_SHAPE[2])))                                            curr_state_arr = self.load_state_np(temp_filepath)                    state_arr_torch = np2torch(curr_state_arr)                                        all_states.append(state_arr_torch)                    corres_rewards.append(curr_reward)                                self.level_tool_states[level_index][tool_num] = torch.stack(all_states, dim = 0)            self.level_tool_rewards[level_index][tool_num] = torch.tensor(corres_rewards)                                        batch_states = self.level_tool_states[level_index][tool_num]        batch_rewards =  self.level_tool_rewards[level_index][tool_num]                return batch_states, batch_rewards                                def collect_heat_map(self, tool_num: int, level_index: int):                all_action_values = np.zeros((END_ROWS, END_COLS))                        for row in range(END_ROWS):            for col in range(END_COLS):                print(row,col)                rew = self.sample_only(level_index, tool_num, row, col)                if rew < 0:                    all_action_values[row][col] = rew                    continue                                img_arr = self.env_list[level_index].render()[0]                state_arr_np = add_coord_conv_channels_np(to_state_np(img_arr))                state_arr_torch = np2torch(state_arr_np)                                pred_rew = self.Q_network(state_arr_torch)                                all_action_values[row][col] = pred_rew.item()                expanded_arr = np.zeros((600,600))                rows = END_ROWS        cols = END_COLS        ratio_r = int(600//rows)        ratio_c = int(600//cols)                for row in range(END_ROWS):            for col in range(END_COLS):                value = all_action_values[row][col]                srow = row*ratio_r                scol = col*ratio_c                for ar in range(ratio_r):                    for ac in range(ratio_c):                        expanded_arr[srow+ar][scol+ac] = value                            return expanded_arr        def load_state_np(self, filepath):        curr_state_arr = np.loadtxt(filepath)        final_state_arr = np.reshape(curr_state_arr, INPUT_STATE_SHAPE)        return final_state_arr        def sample_best(self, level_index: int):        level_val = self.levels_list[level_index]                best_value = -(10 ** 30)        best_tuple = (-1,-1,-1) # (Tool_num, row, col)                for tool_num in range(3):            filepath = tool_level_rewards_filepath(tool_num, level_val)                        use_rewards = np.loadtxt(filepath)            np_rewards = self.modify_rewards(use_rewards)                        for row in range(END_ROWS):                for col in range(END_COLS):                    rew = np_rewards[row][col]                    if rew < 0:                        continue                                        temp_filepath = tool_level_row_col_filepath(tool_num, level_val, row, col)                    curr_state_arr = self.load_state_np(temp_filepath)                    state_arr_torch = np2torch(curr_state_arr)                    state_arr_torch = state_arr_torch.unsqueeze(dim = 0)                                        pred_rew = self.Q_network(state_arr_torch)[0][0]                                        if pred_rew > best_value:                        best_value = pred_rew                        best_tuple = (tool_num, row, col)                                tool_num, row, col = best_tuple        x, y = row_col_to_x_y(row, col)        action = np.array([int(round(tool_num)), x, y])        obtained_reward = self.sample_only(level_index, tool_num, row, col)                print("Obtained Reward for level: " + str(self.levels_list[level_index]))        print("Action: " + str(action))        print("Reward: " + str(obtained_reward))                return obtained_reward            def save_all(self, tool_num: int, NITER_VAL, level_index):        level_val = self.levels_list[level_index]        heatmap = self.collect_heat_map(tool_num, level_index)        temp_filepath_heatmap_vals = "data/heatmapDataFull/" + str(level_val) + "/heatmapvals_tool_" + str(int(tool_num)) + "_" + str(int(NITER_VAL)) + ".txt"        temp_filepath_params = "data/modelparamsFull/" + str(int(NITER_VAL)) + ".txt"        temp_filepath_heatmap_png = "data/heatmapDataFull/" + str(level_val) + "/heatmap_tool_" + str(int(tool_num)) + "_" + str(int(NITER_VAL)) + ".png"                np.savetxt(temp_filepath_heatmap_vals, heatmap)        torch.save(self.Q_network.state_dict(), temp_filepath_params)        plot_heatmap(heatmap, temp_filepath_heatmap_png)            def save_everything(self, NITER_VAL):        for level_index, level in enumerate(self.levels_list):            print("Saving level " + str(level))            if level > 0: break            for tool in range(3):                print("Saving tool " + str(tool))                self.save_all(tool, NITER_VAL, level_index)                    def save_model(self, NITER_VAL):        temp_filepath_params = "data/modelparamsFull/" + str(int(NITER_VAL)) + ".txt"        torch.save(self.Q_network.state_dict(), temp_filepath_params)            def test_single_tool_level(self, level_index: int, tool_num: int):        level_val = self.levels_list[level_index]                filepath = tool_level_rewards_filepath(tool_num, level_val)        use_rewards = np.loadtxt(filepath)                indexes_li = []        for row in range(END_ROWS):            for col in range(END_COLS):                if use_rewards[row][col] >= 0:                    indexes_li.append((row,col))                            batch_states = self.level_tool_states[level_index][tool_num]                            print("Trying level ", level_val, " with tool ", tool_num)                       found = False        attempts = 0        while not(found) and attempts < 10:            pred_vals = self.Q_network(batch_states).squeeze(dim=-1)            print(pred_vals.size())            max_index = torch.argmax(pred_vals)            max_row, max_col = indexes_li[max_index]            obtained_reward = use_rewards[max_row][max_col]            print("Attempt: ", max_row, max_col)            print("Estimated reward: ", pred_vals[max_index])            print("Reward: ", obtained_reward)                        found = obtained_reward > 0            attempts += 1                        if obtained_reward == 0: obtained_reward -= 1000                        self.backprop_single(self.level_tool_states[level_index][tool_num][max_index], obtained_reward)                                return attempts            def test_run(self):                LAST_ITER = 10        self.init_model()        self.Q_network.load_state_dict(torch.load("data/modelparamsFull/" + str(int(LAST_ITER)) + ".txt"))                for level_index, level_val in enumerate(self.levels_list):            print("Loading level ", level_val)            for tool_num in range(3):                print("Loading tool ", tool_num)                rewards_filepath = tool_level_batch_rewards_filepath(tool_num, level_val)                states_filepath = tool_level_batch_states_filepath(tool_num, level_val)                self.level_tool_rewards[level_index][tool_num] = torch.load(rewards_filepath)                self.level_tool_states[level_index][tool_num] = torch.load(states_filepath)                for level_index, level_val in enumerate(self.levels_list):            for tool_num in range(1,3):                self.Q_network.load_state_dict(torch.load("data/modelparamsFull/" + str(int(LAST_ITER)) + ".txt"))                attempts = self.test_single_tool_level(level_index, tool_num)                print("Took ", attempts, " attempts")                    def run(self):        """        Apply procedures of training for a PG.        """                START_ITER = 6        END_ITER = 10                self.init_model()        self.Q_network.load_state_dict(torch.load("data/modelparamsFull/" + str(int(START_ITER)) + ".txt"))                for level_index, level_val in enumerate(self.levels_list):            print("Loading level ", level_val)            for tool_num in range(3):                print("Loading tool ", tool_num)                rewards_filepath = tool_level_batch_rewards_filepath(tool_num, level_val)                states_filepath = tool_level_batch_states_filepath(tool_num, level_val)                self.level_tool_rewards[level_index][tool_num] = torch.load(rewards_filepath)                self.level_tool_states[level_index][tool_num] = torch.load(states_filepath)                for iteration in range(START_ITER+1, END_ITER+1):            print("Backproping iteration: " + str(iteration))            for level_index, level_val in enumerate(self.levels_list):                for tool_num in range(3):                    print("Level " + str(level_val))                    print("Tool num " + str(tool_num))                    batch_states = self.level_tool_states[level_index][tool_num]                    batch_rewards = self.level_tool_rewards[level_index][tool_num]                                        loss = self.backprop_batch_full(batch_states, batch_rewards)                    print("Loss: " + str(loss))            self.save_model(iteration)                """        for level_index, level_val in enumerate(self.levels_list):            print("Collecting for level: " + str(level_val))            for tool_num in range(3):                print("Currently collecting tool " + str(tool_num))                states, rewards = self.collect_level_tool_states_rewards(level_index, tool_num)                states_filepath = tool_level_batch_states_filepath(tool_num, level_val)                rewards_filepath = tool_level_batch_rewards_filepath(tool_num, level_val)                torch.save(states, states_filepath)                torch.save(rewards, rewards_filepath)                        """                """        self.init_model()                        self.single_run_full(0,0,-0.8,0.8)        #self.Q_network.load_state_dict(torch.load("model_params.txt"))                self.save_model(0)                for iteration in range(1,NITERS+1):            print("ITERATION " + str(iteration))            for level_index, env_level in enumerate(self.env_list):                print("Training level: " + str(self.levels_list[level_index]))                for tn in range(3):                    self.train_n_iters(niters = 1, level_index = level_index, tool_num = tn)            self.save_model(iteration)                for level_index, env_level in enumerate(self.env_list):            rew = self.sample_best(level_index)                    """                """        self.save_all(0, 0, self.level)                for i in range(1,6):            TEMP_NITERS = 10            TOTAL_ITERS = i * 10                        if i == 1:                for j in range(10):                    self.train_n_iters(niters = 1, tool_num = 0)                    self.save_all(tool_num = 0, NITER_VAL = j+1, level_val = self.level)                continue                        self.train_n_iters(niters = TEMP_NITERS, tool_num = 0)            self.save_all(tool_num = 0, NITER_VAL = TOTAL_ITERS, level_val = self.level)                self.sample_best(0)        #self.single_run()        """        #temp_filepath_heatmap_vals = "data/heatmapData/heatmapvals_tool0_" + str(int(self.level)) + "_" + str(NITERS) + ".txt"        #temp_filepath_params = "data/modelparams/model_params_tool0_" + str(int(self.level)) + "_" + str(NITERS) + ".txt"        #temp_filepath_heatmap_png = "data/heatmapData/heatmap_tool0_pic_"  + str(int(self.level)) + "_" + str(NITERS) + ".png"                #np.savetxt(temp_filepath_heatmap_vals, heatmap)        #torch.save(self.Q_network.state_dict(), temp_filepath_params)        #plot_heatmap(heatmap, temp_filepath_heatmap_png)        if __name__ == "__main__":    parser = argparse.ArgumentParser()    parser.add_argument(        "--seed",        type=int,        required = False,        default=1,        help="seed",    )    parser.add_argument(        "--counterfactual",        action='store_true',        help="counterfactual",    )    parser.add_argument(        "--shaped_reward",        action='store_true',        help="shaped reward",    )    parser.add_argument(        "--object_prior",        action='store_true',        help="object prior",    )    parser.add_argument(        "--baseline",        action='store_true',        help="subtract average reward",    )    parser.add_argument(        "--epochs",        type=int,        required=False,        default=100,        help="training_length",    )    parser.add_argument(        "--batch_size",        type=int,        required=False,        default=10,        help="batch size ",    )    parser.add_argument(        "--eval_frq",        type=int,        required=False,        default=5,        help="batch size",    )    parser.add_argument(        "--eval_trials",        type=int,        required=False,        default=20,        help="number of times to run during eval",    )    parser.add_argument(        "--level",        type=int,        required=False,        default=0,        help="level of the game you play",    )    parser.add_argument(        "--name",        type=str,        required=False,        default="full_algorithm",        help="name of algorithm run",    )    parser.add_argument(        "--exp_dir",        type=str,        required=False,        default="experiments/initial_tests",        help="name of algorithm run",    )    args = parser.parse_args()    env_list = [ToolEnv(environment = lev, shaped = args.shaped_reward) for lev in range(-2,4)]    test_env_list = [ToolEnv(environment = lev, shaped = args.shaped_reward) for lev in range(4,7)]    dqn = DQN(env_list, exp_dir = args.exp_dir, name = args.name, levels_list = [i for i in range(-2,4)], test_env_list = test_env_list, test_levels_list = [i for i in range(4,7)])    if args.seed == 1: dqn.run()    else: dqn.test_run()            