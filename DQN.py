import numpy as npimport torchfrom torch import nn as nnimport gymimport osfrom network_utils import np2torchfrom base_gaussian_policy import GaussianToolPolicyfrom environment.simulator import ToolEnvimport matplotlib.pyplot as pltimport argparseimport imageioimport tqdmfrom collections import dequefrom PIL import ImageDEVICE = "cuda" if torch.cuda.is_available() else "cpu"END_ROWS = 50END_COLS = 50INPUT_STATE_SHAPE = (7,600,600)NITERS = 800def compute_linear_input(height, width, conv_layers, model_version):    mod_channels = 32/(2 ** (conv_layers-1))    if model_version == 'base':        mod_height = height - (2 * conv_layers)        mod_width = width - (2 * conv_layers)    else:        mod_height = height/(2 ** conv_layers)        mod_width = width/(2 ** conv_layers)    return int(round(mod_channels * mod_height * mod_width))def add_coord_conv_channels_np(img_arr):    _, rows, cols = np.shape(img_arr)    base_row = np.reshape(np.arange(cols) + 1, (1,-1))    ones_arr_rows = np.zeros((rows,1)) + 1    x_coord_channel = np.expand_dims(np.matmul(ones_arr_rows, base_row), axis=0)    y_coord_channel = np.expand_dims(np.matmul(base_row.T, ones_arr_rows.T), axis=0)    #print(np.shape(x_coord_channel))    #print(np.shape(y_coord_channel))    return np.concatenate((x_coord_channel[:,::-1], y_coord_channel[::-1],x_coord_channel, y_coord_channel, img_arr), axis = 0)def to_image_np(state):    return np.swapaxes(np.swapaxes(state, 0, 1), 1, 2)def to_state_np(image_arr):    return np.swapaxes(np.swapaxes(image_arr, 1, 2), 0, 1)def to_image_torch(state):    return torch.swapaxes(torch.swapaxes(state, 0, 1), 1, 2)def to_state_torch(image_arr):    return torch.swapaxes(torch.swapaxes(image_arr, 1, 2), 0, 1)def save_arr_to_file(image_arr, filepath):    into_file_format = image_arr.astype(np.uint8)    im = Image.fromarray(into_file_format)    im.save(filepath)    def row_col_to_x_y(row, col):    return float(col - END_ROWS//2)/(END_COLS/2), -float(row - END_COLS//2)/(END_ROWS/2)def row_col_to_index(row, col):    return row * END_COLS + coldef index_to_row_col(index):    row = int(round(index))//END_COLS    col = int(round(index))%END_COLS    return int(row), int(col)def plot_heatmap(arr, filepath):    plt.clf()    plt.imshow(arr, cmap='hot', interpolation='nearest')    plt.savefig(filepath)    plt.clf()    def tool_level_rewards_filepath(tool_num: int, level: int):    return "data/level_tool_rewards/" + str(int(level)) + "_" + str(int(tool_num)) + "_" + str(END_ROWS) + "_" + str(END_COLS) + ".txt"class Q_Network(nn.Module):    def __init__(self, state_input_size, num_actions):        super().__init__()        (channels, height, width) = state_input_size        linear_medium_size = 200 * 200                print("in to linear layer size: " + str(compute_linear_input(height, width, 3, 'conv')))                layers_small = [nn.Conv2d(channels, 4, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.MaxPool2d(kernel_size = 2, stride = 2),                         nn.Conv2d(4, 2, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.MaxPool2d(kernel_size = 2, stride = 2),                         nn.Conv2d(2, 1, kernel_size = 3, stride = 1, padding = 'same'),                         nn.ReLU(),                        nn.Flatten(),                         nn.Linear(150*150,100*100),                         nn.ReLU(),                         nn.Linear(100*100,50*50)]                """        layers_shrink = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(16, 8, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(8, 4, kernel_size = 3, stride = 1, padding = 'same'),                   nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 3, 'conv'), linear_medium_size),                   nn.ReLU(),                   nn.Linear(linear_medium_size, num_actions)                  ]                        layers_3conv = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(16, 8, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(8, 4, kernel_size = 3, stride = 1, padding = 'same'),                   nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 3, 'conv'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]                layers_2conv = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                   nn.MaxPool2d(kernel_size = 2, stride = 2),                   nn.Conv2d(16, 8, kernel_size = 3, stride = 1, padding = 'same'),                   nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 2, 'conv'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]                layers_1conv = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 'same'),                    nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 1, 'conv'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]                layers_base = [nn.Conv2d(channels, 16, kernel_size = 3, stride = 1, padding = 0),                    nn.ReLU(),                   nn.Flatten(),                   nn.Linear(compute_linear_input(height, width, 1, 'base'), 100 * 100),                   nn.ReLU(),                   nn.Linear(100 * 100, num_actions)                  ]        """        self.learning_rate = 0.1        self.model = nn.Sequential(*layers_small)        self.model = self.model.to(DEVICE)        self.optimizer = torch.optim.Adam(params = self.model.parameters(), lr = self.learning_rate)        self.num_actions = num_actions        def forward(self, input_state):        input_state = input_state.to(DEVICE)        return self.model(input_state)        def backprop_single(self, state, action, true_value):        self.optimizer.zero_grad()        computed_value = torch.sum(self.model(state) * torch.nn.functional.one_hot(torch.tensor(action).to(torch.int64), self.num_actions))        mse_loss = (computed_value - true_value) ** 2        mse_loss.backward()        self.optimizer.step()        return mse_loss.item()        def backprop_batch_full(self, state, all_rewards):        self.optimizer.zero_grad()        action_values = self.model(state)[0]        loss = 0        for index in range(len(action_values)):            row, col = index_to_row_col(index)            true_reward = all_rewards[row][col]            comp_reward = action_values[index]            loss += ((true_reward - comp_reward) ** 2)        loss.backward()        self.optimizer.step()        return loss.item()class DQN(object):    """    Class for implementing a DQN    """    def __init__(self, env, seed, exp_dir, name, curr_level):        self.env = env        #self.env.seed(self.seed)        # self.lr = 3e-2        self.lr = 0.1 #0.1 #1.5 #0.5        self.exp_dir = exp_dir        self.name = name                self.state_shape = INPUT_STATE_SHAPE        self.num_actions = END_ROWS * END_COLS        self.level = curr_level    def copy_to_target(self):        self.target_network.model.load_state_dict(self.Q_network.model.state_dict())            def get_action_values(self, input_state, model_type = 'target'):        if model_type == 'target' or model_type == 'target_network':            return self.target_network(input_state)        elif model_type == 'q_network':            return self.Q_network(input_state)        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")                def backprop_single(self, state, action, target_value, model_type = 'q_network'):        if model_type == 'q_network':            use_model = self.Q_network        elif model_type == 'target' or model_type == 'target_network':            use_model = self.target_network        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")            print("FOR BACKPROP THERE ISN'T ANY REASON TO PASS THIS PARAMETER IN")                    loss = use_model.backprop_single(state, action, target_value)        return loss        def backprop_batch_full(self, state, all_rewards, model_type = 'q_network'):        if model_type == 'q_network':            use_model = self.Q_network        elif model_type == 'target' or model_type == 'target_network':            use_model = self.target_network        else:            print("PLEASE GIVE ETHER target OR q_network AS THE model_type PARAMETER")            print("FOR BACKPROP THERE ISN'T ANY REASON TO PASS THIS PARAMETER IN")                    loss = use_model.backprop_batch_full(state, all_rewards)        return loss        def init_model(self):        self.Q_network = Q_Network(self.state_shape, self.num_actions)        #self.target_network = Q_Network(self.state_shape, self.num_actions)            """    def init_policy(self):        device = "cuda" if torch.cuda.is_available() else "cpu"        self.policy = GaussianToolPolicy(ntools = 3, nsteps = args.epochs, object_prior = self.env.object_prior_dict, device = device) #arbitrary steps for now; should converge very quickly        self.policy.to(device)        # self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=self.lr)        self.optimizer = torch.optim.SGD(self.policy.parameters(), lr=self.lr)        # self.optimizer = torch.optim.SGD(self.policy.parameters(), lr=self.lr)    """        def init_averages(self):        """        You don't have to change or use anything here.        """        self.avg_reward = 0.0        self.max_reward = 0.0        self.std_reward = 0.0        self.eval_reward = 0.0    def update_averages(self, rewards, scores_eval):        """        Update the averages.        You don't have to change or use anything here.        Args:            rewards: deque            scores_eval: list        """        self.avg_reward = np.mean(rewards)        self.max_reward = np.max(rewards)        self.std_reward = np.sqrt(np.var(rewards) / len(rewards))        if len(scores_eval) > 0:            self.eval_reward = scores_eval[-1]    def record_summary(self, t):        pass    def sample_path(self, env, num_episodes=None, prior = False):        """        Sample paths (trajectories) from the environment.        Args:            num_episodes: the number of episodes to be sampled                if none, sample one batch (size indicated by config file)            env: open AI Gym envinronment        Returns:            paths: a list of paths. Each path in paths is a dictionary with                path["observation"] a numpy array of ordered observations in the path                path["actions"] a numpy array of the corresponding actions in the path                path["reward"] a numpy array of the corresponding rewards in the path            total_rewards: the sum of all rewards encountered during this "path"        You do not have to implement anything in this function, but you will need to        understand what it returns, and it is worthwhile to look over the code        just so you understand how we are taking actions in the environment        and generating batches to train on.        """        episode_rewards = []        paths = []        # print("COLLECTING BATCH")        niters = self.batch_size if num_episodes is None else num_episodes        for t in tqdm.tqdm(range(niters)):            if prior:                action = self.policy.act(prior_only=True)            else:                action = self.policy.act()            self.policy.hold()  # this just means that we will not change (either prior or policy)            if args.counterfactual:                #COUNTERFACTUAL SAMPLING                for i in range(3):                    env.reset()                    action[0] = i                    reward = env.step(action)                    count = 0                    while reward is None: #this is done for illegal moves                        if prior:                            action = self.policy.act(prior_only = True)                        else:                            action = self.policy.act()                        action[0] = i                        reward = env.step(action)                        count += 1                        if count == 100: #on th 10th try, reward automatically is 0                            print('UNSUCCESSFUL, GIVING REWARD OF ZERO ***')                            reward = 0.0                            break                    episode_rewards.append(reward)                    paths.append({                        "reward": reward,                        "action": action.copy(),                    })                self.policy.reset_prior()  # reset the prior state after every sampling            else:                env.reset()                reward = env.step(action)                count = 0                self.policy.hold()                while reward is None:  # this is done for illegal moves                    action = self.policy.act()                    reward = env.step(action)                    count += 1                    if count == 100:  # on th 10th try, reward automatically is 0                        print('UNSUCCESSFUL, GIVING REWARD OF ZERO ***')                        reward = 0.0                        break                self.policy.reset_prior()                episode_rewards.append(reward)                paths.append({                    "reward": reward,                    "action": action.copy(),                })        return paths, episode_rewards    ##### EQUIVALENT TO BACKPROP SINGLE AND NOT NEEDED #####    def update_policy(self, actions, advantages):        actions = np2torch(actions)        advantages = np2torch(advantages)        #######################################################        #########   YOUR CODE HERE - 5-7 lines.    ############        # actions = torch.tensor(actions)        log_probs = self.policy.log_prob(actions) #should be batch size        loss = -torch.dot(log_probs, advantages) / advantages.shape[0]        print(loss.item())        self.optimizer.zero_grad()        loss.backward()        # print(self.policy.log_std.grad) #should gbe all populated        # print(self.policy.means.grad) #should be all populated        self.optimizer.step()        # print(loss)        # self.policy.print_repr()        return loss.item()    def train(self):        self.init_averages()        all_total_rewards = (            []        )  # the returns of all episodes samples for training purposes        success_eval = list()        paths, total_rewards = self.sample_path(self.env, num_episodes=10, prior = True)        actions = np.stack([path["action"] for path in paths])        returns = np.stack([path["reward"] for path in paths]) #only one step        rewards_buffer = deque(maxlen=50)        rewards_buffer.extend(returns.tolist())        self.env.visualize_actions(actions, "action_distr.png")        rwds, succ = self.evaluate(0)        print("LOW NOISE EVAL: ", rwds, succ)        success_eval.append(succ)        print('BURN-IN')        last_loss = 1000        baseline = sum(rewards_buffer) / len(rewards_buffer) if args.baseline else 0        for t in range(100):            loss = self.update_policy(actions, returns - baseline)            if last_loss - loss < 0.001:                break            last_loss = loss        for t in range(args.epochs):            # collect a minibatch of samples            paths, total_rewards = self.sample_path(self.env)            all_total_rewards.extend(total_rewards)            actions = np.stack([path["action"] for path in paths])            returns = np.stack([path["reward"] for path in paths]) #only true because it's one-step return            rewards_buffer.extend(returns.tolist())            baseline = sum(rewards_buffer) / len(rewards_buffer) if args.baseline else 0            print("MODIFIED REWARDS" , len(rewards_buffer), returns - baseline)            self.update_policy(actions, returns - baseline)            self.policy.anneal_epsilon(t)            avg_reward = np.mean(total_rewards)            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))            msg = "[ITERATION {}]: Average reward: {:04.2f} +/- {:04.2f}".format(                t, avg_reward, sigma_reward            )            print(msg)            # RENDERING FOR US            if t % args.eval_frq == 0 and t > 0:                rwds, succ= self.evaluate(t)                print("LOW NOISE EVAL: ", rwds, succ)                success_eval.append(succ)            # self.env.render(t)                    # PLOTTING RESULTS        fig, ax = plt.subplots()        print(success_eval)        x_axis = np.arange(0, args.epochs, args.eval_frq)        ax.plot(x_axis, success_eval)        ax.set_ylabel("Success Rate")        ax.set_xlabel("Epochs")        ax.set_title("Performance on Basic Environment")        fig.savefig(self.exp_dir + f"/{self.name}_level{args.level}_{self.seed}.png")        np.save(self.exp_dir + f"/{self.name}_level{args.level}_{self.seed}", success_eval)        # plt.show()    def evaluate(self, step):        avg_reward = 0        avg_success = 0        writer = imageio.get_writer(self.exp_dir + f"/{self.name}_level{args.level}_{self.seed}_{step}.mp4", fps=20)        std = np.exp(self.policy.log_std.detach().cpu().numpy())        means = self.policy.means.detach().cpu().numpy()        self.env.visualize_distributions(means, std, self.exp_dir + f"/{self.name}_level{args.level}_{self.seed}_{step}.png")        for i in tqdm.tqdm(range(args.eval_trials)):            self.env.reset()            rwd = None            count = 0            while rwd is None and count < 10:                action = self.policy.act(low_noise = True)                rwd = self.env.step(action, display = False)# (i % 5 == 0))                count += 1                if count == 10:                    print("gave up!")                    rwd = 0.0            avg_reward += rwd            avg_success += 1 if rwd > 0.99 else 0            img_stack = self.env.render()            if img_stack is not None:                for f in range(img_stack.shape[0]):                    writer.append_data(img_stack[f])        writer.close()        return avg_reward / args.eval_trials, avg_success / args.eval_trials            def single_run_full(self, tool_num, pos_x, pos_y):        env.reset()        img_arr = env.img.copy()                save_arr_to_file(img_arr, "pre_tool_env.png")                action = np.array([tool_num,pos_x,pos_y])        print("Action: ", action)                reward = env.step(action)        print("Reward" + str(reward))                post_tool_imgs = env.render()                for index in range(len(post_tool_imgs)):            filepath = "temp_imgs/img-" + str(index) + ".png"            curr_img = post_tool_imgs[index]            save_arr_to_file(curr_img, filepath)                    post_tool_img_arr = post_tool_imgs[-1]        #print(post_tool_img_arr)        #print(np.shape(post_tool_img_arr))        save_arr_to_file(post_tool_img_arr, "post_tool_env.png")                env.reset()            def sample_and_train(self, tool_num, row, col):        env.reset()        img_arr = env.img.copy()                        np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()                x, y = row_col_to_x_y(row, col)        action = np.array([int(round(tool_num)), x, y])                reward = env.step(action)        if reward is None:            reward = -1                    reward *= 1000        if reward > 0:            reward *= 10                index = row_col_to_index(row, col)        loss = self.backprop_single(torch_state, index, reward)                return loss        def sample_only(self, tool_num, row, col):        env.reset()                x, y = row_col_to_x_y(row, col)        #print(x,y)        action = np.array([int(round(tool_num)), x, y])                reward = env.step(action)        if reward is None:            reward = -1                    reward *= 1000        if reward > 0:            reward *= 10                    return reward        def sample_all_rewards(self, tool_num: int):        results = np.zeros((END_ROWS, END_COLS))        print("SAMPLING ALL REWARDS")        for row in range(END_ROWS):            print()            print("Row: " + str(row))            print("Cols: ")            for col in range(END_COLS):                if col%15 == 0:                    print(col)                reward = self.sample_only(tool_num, row, col)                results[row][col] = reward        return results        def sample_all_n_times(self, niters: int, tool_num: int):        for iteration in range(1,niters+1):            print("Iteration: ", iteration)            for row in range(80):                print(row)                for col in range(80):                    self.sample_and_train(tool_num, row, col)            for row in range(80,150,5):                print(row)                for col in range(80,150,5):                    self.sample_and_train(tool_num, row, col)                      def modify_rewards(self, np_arr):        #np_arr[np_arr == 0] = -5        #np_arr[np_arr == -1000] = -10        #np_arr[np_arr == 10000] = 25        return np_arr                        def train_n_iters(self, niters: int, tool_num: int):        level = int(self.level)        filepath = tool_level_rewards_filepath(tool_num, level)        if not(os.path.exists(filepath)):            rewards = self.sample_all_rewards(tool_num)            np.savetxt(filepath, rewards)                use_rewards = np.loadtxt(filepath)        mod_rewards = self.modify_rewards(use_rewards)        all_rewards = np2torch(mod_rewards)                env.reset()        img_arr = env.img.copy()                        np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()                for iteration in range(1, niters+1):            print("Backproping iteration: " + str(iteration))            loss = self.backprop_batch_full(torch_state, all_rewards)            print("Loss: " + str(loss))                                        def collect_heat_map(self, tool_num: int):        env.reset()        img_arr = env.img.copy()                        np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()                all_action_values = self.get_action_values(torch_state, model_type = "q_network")[0]                expanded_arr = np.zeros((600,600))                rows = END_ROWS        cols = END_COLS        ratio_r = int(600//rows)        ratio_c = int(600//cols)                for index, value_item in enumerate(all_action_values):            row, col = index_to_row_col(index)            srow = row*ratio_r            scol = col*ratio_c            for ar in range(ratio_r):                for ac in range(ratio_c):                    expanded_arr[srow+ar][scol+ac] = value_item.item()                            env.reset()        return expanded_arr        def sample_best(self, tool_num: int):        env.reset()        img_arr = env.img.copy()                save_arr_to_file(img_arr, "pre_tool_env.png")                np_state = add_coord_conv_channels_np(to_state_np(img_arr))        torch_state = np2torch(np_state).float()        action_values = self.Q_network(torch_state)        action_values_np = action_values.detach().numpy()        np.savetxt("actvals.txt", action_values_np)        best_index = torch.argmax(action_values).item()        best_row, best_col = index_to_row_col(best_index)        best_x, best_y = row_col_to_x_y(best_row, best_col)                print("BEST POSITION RUN INFORMATION")        self.single_run_full(tool_num, best_x, best_y)            def save_all(self, tool_num: int, NITER_VAL, level_val):        heatmap = self.collect_heat_map(tool_num)        temp_filepath_heatmap_vals = "data/heatmapData/heatmapvals_tool_" + str(int(tool_num)) + "_" + str(int(level_val)) + "_" + str(int(NITER_VAL)) + ".txt"        temp_filepath_params = "data/modelparams/model_params_tool_" + str(int(tool_num)) + "_" + str(int(level_val)) + "_" + str(int(NITER_VAL)) + ".txt"        temp_filepath_heatmap_png = "data/heatmapData/heatmap_tool_" + str(int(tool_num)) + "_"  + str(int(level_val)) + "_" + str(int(NITER_VAL)) + ".png"                np.savetxt(temp_filepath_heatmap_vals, heatmap)        torch.save(self.Q_network.state_dict(), temp_filepath_params)        plot_heatmap(heatmap, temp_filepath_heatmap_png)            def run(self):        """        Apply procedures of training for a PG.        """                self.init_model()        self.init_averages()        self.single_run_full(0,-0.8,0.8)        #self.Q_network.load_state_dict(torch.load("model_params.txt"))                self.save_all(0, 0, self.level)                for i in range(1,6):            TEMP_NITERS = 10            TOTAL_ITERS = i * 10                        if i == 1:                for j in range(10):                    self.train_n_iters(niters = 1, tool_num = 0)                    self.save_all(tool_num = 0, NITER_VAL = j+1, level_val = self.level)                continue                        self.train_n_iters(niters = TEMP_NITERS, tool_num = 0)            self.save_all(tool_num = 0, NITER_VAL = TOTAL_ITERS, level_val = self.level)                self.sample_best(0)        #self.single_run()                #temp_filepath_heatmap_vals = "data/heatmapData/heatmapvals_tool0_" + str(int(self.level)) + "_" + str(NITERS) + ".txt"        #temp_filepath_params = "data/modelparams/model_params_tool0_" + str(int(self.level)) + "_" + str(NITERS) + ".txt"        #temp_filepath_heatmap_png = "data/heatmapData/heatmap_tool0_pic_"  + str(int(self.level)) + "_" + str(NITERS) + ".png"                #np.savetxt(temp_filepath_heatmap_vals, heatmap)        #torch.save(self.Q_network.state_dict(), temp_filepath_params)        #plot_heatmap(heatmap, temp_filepath_heatmap_png)                # record one game at the beginning        # model        #self.init_policy()        #self.init_averages()        #self.train()        # record one game at the endif __name__ == "__main__":    parser = argparse.ArgumentParser()    parser.add_argument(        "--seed",        type=int,        required = False,        default=1,        help="seed",    )    parser.add_argument(        "--counterfactual",        action='store_true',        help="counterfactual",    )    parser.add_argument(        "--shaped_reward",        action='store_true',        help="shaped reward",    )    parser.add_argument(        "--object_prior",        action='store_true',        help="object prior",    )    parser.add_argument(        "--baseline",        action='store_true',        help="subtract average reward",    )    parser.add_argument(        "--epochs",        type=int,        required=False,        default=100,        help="training_length",    )    parser.add_argument(        "--batch_size",        type=int,        required=False,        default=10,        help="batch size ",    )    parser.add_argument(        "--eval_frq",        type=int,        required=False,        default=5,        help="batch size",    )    parser.add_argument(        "--eval_trials",        type=int,        required=False,        default=20,        help="number of times to run during eval",    )    parser.add_argument(        "--level",        type=int,        required=False,        default=0,        help="level of the game you play",    )    parser.add_argument(        "--name",        type=str,        required=False,        default="full_algorithm",        help="name of algorithm run",    )    parser.add_argument(        "--exp_dir",        type=str,        required=False,        default="experiments/initial_tests",        help="name of algorithm run",    )    args = parser.parse_args()    env = ToolEnv(environment = args.level, shaped = args.shaped_reward)    dqn = DQN(env, seed = args.seed, exp_dir = args.exp_dir,                        name = args.name, curr_level = args.level)    dqn.run()